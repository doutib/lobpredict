{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from multiprocessing import cpu_count\n",
    "from datetime import timedelta\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layers_nnet(X_train,\n",
    "                    Y_train,\n",
    "                    X_test,\n",
    "                    Y_test,\n",
    "                    method1=\"Tanh\",\n",
    "                    neurons1=5,\n",
    "                    method2=\"\",\n",
    "                    neurons2=0,\n",
    "                    decay=0.0001,\n",
    "                    learning_rate=0.001,\n",
    "                    n_iter=25,\n",
    "                    random_state=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train       : pandas data frame\n",
    "        data frame of features for the training set\n",
    "    Y_train       : pandas data frame\n",
    "        data frame of labels for the training set\n",
    "    X_test        : pandas data frame\n",
    "        data frame of features for the test set\n",
    "    Y_test        : pandas data frame\n",
    "        data frame of labels for the test set\n",
    "    method1       : str\n",
    "        method used for the first layer\n",
    "    neurons1      : int\n",
    "        number of neurons of the first layer\n",
    "    method2       : None\n",
    "        method used for the first layer\n",
    "    neurons2      : int\n",
    "        number of neurons of the first layer\n",
    "    decay         : float\n",
    "        weight decay\n",
    "    learning_rate : float\n",
    "        learning rate\n",
    "    n_iter        : int\n",
    "        number of iterations\n",
    "    random_state  : int\n",
    "        seed for weight initialization\n",
    "        \n",
    "    Result:\n",
    "    -------\n",
    "    numpy array\n",
    "        logloss    : averaged logarithmic loss\n",
    "        miss_err   : missclassification error rate\n",
    "        prec       : precision\n",
    "        recall     : recall\n",
    "        f1         : f1 score\n",
    "        parameters : previous parameters in the order previously specified\n",
    "    \"\"\"\n",
    "\n",
    "    labels = np.unique(Y_train)\n",
    "    \n",
    "    ## # Scale Data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Layers\n",
    "    if neurons2 == 0 :\n",
    "        layers=[Layer(method1, weight_decay = decay, units = neurons1),\n",
    "                Layer(\"Softmax\")]\n",
    "    else:\n",
    "        layers=[Layer(method1, weight_decay = decay, units = neurons1),\n",
    "                Layer(method2, weight_decay = decay, units = neurons2),\n",
    "                Layer(\"Softmax\")]\n",
    "        \n",
    "    ## # Run nnet\n",
    "    # Define classifier\n",
    "    nn = Classifier(layers,\n",
    "                    learning_rate=learning_rate,\n",
    "                    random_state=random_state,\n",
    "                    n_iter=n_iter)\n",
    "    # Fit\n",
    "    nn.fit(X_train, Y_train)\n",
    "    # Predict\n",
    "    Y_hat = nn.predict(X_test)\n",
    "    Y_probs = nn.predict_proba(X_test)\n",
    "    \n",
    "    ## # Misclassification error rate\n",
    "    miss_err = 1-accuracy_score(Y_test, Y_hat)\n",
    "    ## # Log Loss\n",
    "    eps = 10^(-15)\n",
    "    logloss = log_loss(Y_test, Y_probs, eps = eps)\n",
    "    \n",
    "    ## # Precision\n",
    "    prec = precision_score(y_true=Y_test, y_pred=Y_hat, labels=labels, average='micro')\n",
    "    ## # Recal\n",
    "    recall = recall_score(y_true=Y_test, y_pred=Y_hat, labels=labels, average='micro') \n",
    "    ## # F1\n",
    "    f1 = f1_score(y_true=Y_test, y_pred=Y_hat, labels=labels, average='micro')\n",
    "    \n",
    "    # Summarized results\n",
    "    result = np.array([logloss,\n",
    "                       miss_err,\n",
    "                       prec,\n",
    "                       recall,\n",
    "                       f1,\n",
    "                       method1,\n",
    "                       neurons1,\n",
    "                       method2,\n",
    "                       neurons2,\n",
    "                       decay,\n",
    "                       learning_rate,\n",
    "                       n_iter,\n",
    "                       random_state])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def processInput((X_train,Y_train,X_test,Y_test,parameters,index)): \n",
    "    # Define parameters names\n",
    "    method1,neurons1,method2,neurons2,decay,learning_rate,n_iter,random_state=parameters[index]\n",
    "    \n",
    "    # Run nnet\n",
    "    result = two_layers_nnet(X_train,\n",
    "                             Y_train,\n",
    "                             X_test,\n",
    "                             Y_test,\n",
    "                             method1,\n",
    "                             neurons1,\n",
    "                             method2,\n",
    "                             neurons2,\n",
    "                             decay,\n",
    "                             learning_rate,\n",
    "                             n_iter,\n",
    "                             random_state)\n",
    "    return result\n",
    "\n",
    "\n",
    "def two_layers_nnet_simulation(X_train,\n",
    "                               Y_train,\n",
    "                               X_test,\n",
    "                               Y_test,\n",
    "                               method1,\n",
    "                               neurons1,\n",
    "                               method2,\n",
    "                               neurons2,\n",
    "                               decay,\n",
    "                               learning_rate,\n",
    "                               n_iter,\n",
    "                               random_state):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    Same parameters as two_layers_nnet, in a list format.\n",
    "    \n",
    "    Result:\n",
    "    ------\n",
    "    List of Lists of results from two_layers_nnet.\n",
    "        One list corresponds to one set of parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Lauching Simulation...')\n",
    "    start = time.time()\n",
    "    \n",
    "    # Combinations\n",
    "    param = np.array([method1,\n",
    "                      neurons1,\n",
    "                      method2,\n",
    "                      neurons2,\n",
    "                      decay,\n",
    "                      learning_rate,\n",
    "                      n_iter,\n",
    "                      random_state])\n",
    "    \n",
    "    parameters = list(itertools.product(*param))\n",
    "    \n",
    "    indexes = range(len(parameters))\n",
    "    print \"Number of sets of parameters: %s.\\n\" %len(parameters)\n",
    "    \n",
    "    print 'Parameters:\\n-----------'\n",
    "    print np.array(parameters)\n",
    "    \n",
    "\n",
    "    # Number of clusters\n",
    "    num_cpu = cpu_count()          \n",
    "    print \"\\nNumber of identified CPUs: %s.\\n\" %num_cpu\n",
    "    num_clusters = min(num_cpu,len(parameters))\n",
    "    \n",
    "    ## # Parallelization\n",
    "    tuples_indexes = tuple([(X_train,Y_train,X_test,Y_test,parameters,index) for index in indexes])\n",
    "\n",
    "    # Start clusters\n",
    "    print 'Start %s clusters.\\n' % num_clusters\n",
    "    print 'Running...'\n",
    "    pool = Pool(processes=num_clusters)\n",
    "    results = pool.map(processInput, tuples_indexes) \n",
    "    pool.terminate()\n",
    "    \n",
    "    # Results\n",
    "    print 'Results:\\n--------'\n",
    "    print results\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print 'End of Simulation.\\nElapsed time: %s' %str(timedelta(seconds=elapsed))\n",
    "    print 'Write into csv...'\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layers_nnet_predict(X_train,\n",
    "                            Y_train,\n",
    "                            X_test,\n",
    "                            method1=\"Tanh\",\n",
    "                            neurons1=5,\n",
    "                            method2=\"\",\n",
    "                            neurons2=0,\n",
    "                            decay=0.0001,\n",
    "                            learning_rate=0.001,\n",
    "                            n_iter=25,\n",
    "                            random_state=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train       : pandas data frame\n",
    "        data frame of features for the training set\n",
    "    Y_train       : pandas data frame\n",
    "        data frame of labels for the training set\n",
    "    X_test        : pandas data frame\n",
    "        data frame of features for the test set\n",
    "    method1       : str\n",
    "        method used for the first layer\n",
    "    neurons1      : int\n",
    "        number of neurons of the first layer\n",
    "    method2       : None\n",
    "        method used for the first layer\n",
    "    neurons2      : int\n",
    "        number of neurons of the first layer\n",
    "    decay         : float\n",
    "        weight decay\n",
    "    learning_rate : float\n",
    "        learning rate\n",
    "    n_iter        : int\n",
    "        number of iterations\n",
    "    random_state  : int\n",
    "        seed for weight initialization\n",
    "        \n",
    "    Result:\n",
    "    -------\n",
    "    tuple of numpy arrays\n",
    "        (predicted classes, predicted probabilities)\n",
    "    \"\"\"\n",
    "\n",
    "    labels = np.unique(Y_train)\n",
    "    \n",
    "    ## # Scale Data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    ## # Split data set into train/test\n",
    "    \n",
    "    # Layers\n",
    "    if neurons2 == 0 :\n",
    "        layers=[Layer(method1, weight_decay = decay, units = neurons1),\n",
    "                Layer(\"Softmax\")]\n",
    "    else:\n",
    "        layers=[Layer(method1, weight_decay = decay, units = neurons1),\n",
    "                Layer(method2, weight_decay = decay, units = neurons2),\n",
    "                Layer(\"Softmax\")]\n",
    "        \n",
    "    ## # Run nnet\n",
    "    # Define classifier\n",
    "    nn = Classifier(layers,\n",
    "                    learning_rate=learning_rate,\n",
    "                    random_state=random_state,\n",
    "                    n_iter=n_iter)\n",
    "    # Fit\n",
    "    nn.fit(X_train, Y_train)\n",
    "    # Predict\n",
    "    Y_hat = nn.predict(X_test)\n",
    "    Y_probs = nn.predict_proba(X_test)\n",
    "    \n",
    "    # Summarized results\n",
    "    result = (Y_hat,Y_probs)\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
